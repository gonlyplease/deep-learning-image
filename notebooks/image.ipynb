{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\n",
    "https://kushalvyas.github.io/BOV.html\n",
    "http://127.0.0.1:5500/image-feature-extraction.html\n",
    "https://www.geeksforgeeks.org/feature-extraction-in-image-processing-techniques-and-applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main goal of this exercise is to get a feeling and understanding on the importance of representation and extraction of information from complex media content, in this case images. \n",
    "\n",
    "1. Start Simple with Colour Histograms\n",
    "2. Explore Key Point-Based Feature Extraction\n",
    "    - Explore Key Point-Based Feature Extraction: Once you’re comfortable with simple features, try using SIFT for key point detection and descriptor extraction. SIFT is robust to scale and rotation, which makes it ideal for finding distinctive features in images.\n",
    "        - Visual Bag of Words: After extracting SIFT features, apply a bag-of-words approach to cluster these descriptors into visual “words.” This converts a variable number of key points into a fixed-length feature vector suitable for classifiers.\n",
    "\n",
    "\n",
    "3. Then, try to use deep learning approaches, such as convolutional neural networks (for images) or recurrent neural networks (for text), or other approaches (but likely not just simple MLPs), and see\n",
    "how your performance differs. Try at least two different architectures, they can be (reusing or be based on )existing, well-known ones.\n",
    "\n",
    "Compare not just the overall measures, but perform a detailed comparison and analysis per class (confusion matrix), to identify if the two approaches lead to different types of errors in the different classes, and also try to identify other patterns.\n",
    "\n",
    "Also perform a detailed comparison of runtime, considering both time for training and testing, including also the feature extraction components.\n",
    "For the datasets you shall work with, pick two text/image datasets, from the list of suggestions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, you can base your DL implementation on the tutorial provided by colleagues at the institute, available at https://github.com/tuwien-musicir/DL_Tutorial/blob/master/Car_recognition.ipynb (you can also check the rest of the repository for interesting code; credit to Thomas Lidy (http://www.ifs.tuwien.ac.at/~lidy/)). Mind also that you will find plenty of examples on how to create and train CNNs / RNNs in various frameworks - tensorflow, keras, pytorch, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# Torch and torchvision \n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001-1359597a978bc4fa.parquet', 'valid': 'data/valid-00000-of-00001-70d52db3c749a935.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/zh-plus/tiny-imagenet/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_color_histogram(image, bins=8):\n",
    "    \"\"\"\n",
    "    Extracts a normalized color histogram from an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Image array in RGB format.\n",
    "        bins (int): Number of bins per channel.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Flattened concatenated normalized histogram for each channel.\n",
    "    \"\"\"\n",
    "    # Ensure image is in uint8 format\n",
    "    if image.dtype != np.uint8:\n",
    "        image = (255 * image).astype(\"uint8\")\n",
    "    \n",
    "    # If image is not in 3 channels, convert (this might happen with grayscale images)\n",
    "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    chans = cv2.split(image)\n",
    "    features = []\n",
    "    \n",
    "    for chan in chans:\n",
    "        # Compute histogram for the channel\n",
    "        hist = cv2.calcHist([chan], [0], None, [bins], [0, 256])\n",
    "        # Normalize histogram\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "        features.extend(hist)\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(X_images, y_labels, dataset_name, bins_values=[8, 16]):\n",
    "    \"\"\"\n",
    "    Run experiments on a given dataset using two histogram configurations.\n",
    "    \n",
    "    Args:\n",
    "        X_images (list): List of images as numpy arrays in RGB format.\n",
    "        y_labels (list or array): Corresponding labels.\n",
    "        dataset_name (str): Name of the dataset (for printing/plotting).\n",
    "        bins_values (list): List of bin counts to try (e.g. [8, 16]).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for bins in bins_values:\n",
    "        print(f\"\\n==== Running experiment on {dataset_name} with {bins} bins per channel ====\")\n",
    "        t0 = time.time()\n",
    "        # Extract features for all images; use tqdm for progress\n",
    "        features = []\n",
    "        for img in tqdm(X_images, desc=\"Extracting features\"):\n",
    "            feat = extract_color_histogram(img, bins=bins)\n",
    "            features.append(feat)\n",
    "        features = np.array(features)\n",
    "        extraction_time = time.time() - t0\n",
    "        print(f\"Feature extraction time: {extraction_time:.2f} sec\")\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, y_labels, test_size=0.3, random_state=42, stratify=y_labels\n",
    "        )\n",
    "        \n",
    "        # Train classifier and measure training time\n",
    "        clf = LogisticRegression(max_iter=500, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "        t_train = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.time() - t_train\n",
    "        print(f\"Training time: {train_time:.2f} sec\")\n",
    "        \n",
    "        # Test classifier and measure prediction time\n",
    "        t_test = time.time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        test_time = time.time() - t_test\n",
    "        print(f\"Testing time: {test_time:.2f} sec\")\n",
    "        \n",
    "        # Compute overall accuracy\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # Compute confusion matrix and classification report\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        results[bins] = {\n",
    "            \"extraction_time\": extraction_time,\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\": test_time,\n",
    "            \"accuracy\": acc,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"classification_report\": report,\n",
    "            \"model\": clf,\n",
    "        }\n",
    "        \n",
    "        # Plot the confusion matrix for visual inspection\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(f\"{dataset_name} Confusion Matrix (bins={bins})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Also display a text report\n",
    "        print(f\"Classification Report (bins={bins}):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Runner Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Tiny ImageNet images:   9%|▉         | 8770/100000 [00:00<00:07, 12521.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Run Tiny ImageNet experiment (if the parquet file is available)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     tiny_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_tiny_imagenet_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTiny ImageNet experiment could not be run:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "Cell \u001b[0;32mIn[10], line 102\u001b[0m, in \u001b[0;36mrun_tiny_imagenet_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(img\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Decode the image bytes using PIL\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         pil_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m         pil_img \u001b[38;5;241m=\u001b[39m pil_img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure RGB format\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pil_img)\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/Image.py:3511\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3508\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3511\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[1;32m   3514\u001b[0m     checked_formats \u001b[38;5;241m=\u001b[39m ID\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/Image.py:3499\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3497\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3498\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3499\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3500\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:873\u001b[0m, in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjpeg_factory\u001b[39m(\n\u001b[1;32m    871\u001b[0m     fp: IO[\u001b[38;5;28mbytes\u001b[39m], filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    872\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JpegImageFile \u001b[38;5;241m|\u001b[39m MpoImageFile:\n\u001b[0;32m--> 873\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mJpegImageFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m         mpheader \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39m_getmp()\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/ImageFile.py:144\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    151\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mv\u001b[39;00m\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:373\u001b[0m, in \u001b[0;36mJpegImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m name, description, handler \u001b[38;5;241m=\u001b[39m MARKER[i]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFDA\u001b[39m:  \u001b[38;5;66;03m# start of scan\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:252\u001b[0m, in \u001b[0;36mDQT\u001b[0;34m(self, marker)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m precision \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    251\u001b[0m     data\u001b[38;5;241m.\u001b[39mbyteswap()  \u001b[38;5;66;03m# the values are always big-endian\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization[v \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m15\u001b[39m] \u001b[38;5;241m=\u001b[39m [data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m zigzag_index]\n\u001b[1;32m    253\u001b[0m s \u001b[38;5;241m=\u001b[39m s[qt_length:]\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m precision \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    251\u001b[0m     data\u001b[38;5;241m.\u001b[39mbyteswap()  \u001b[38;5;66;03m# the values are always big-endian\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization[v \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m15\u001b[39m] \u001b[38;5;241m=\u001b[39m [data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m zigzag_index]\n\u001b[1;32m    253\u001b[0m s \u001b[38;5;241m=\u001b[39m s[qt_length:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment(X_images, y_labels, dataset_name, bins_values=[8, 16]):\n",
    "    \"\"\"\n",
    "    Run experiments on a given dataset using two histogram configurations.\n",
    "    \n",
    "    Args:\n",
    "        X_images (list): List of images as numpy arrays in RGB format.\n",
    "        y_labels (list or array): Corresponding labels.\n",
    "        dataset_name (str): Name of the dataset (for printing/plotting).\n",
    "        bins_values (list): List of bin counts to try (e.g. [8, 16]).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for bins in bins_values:\n",
    "        print(f\"\\n==== Running experiment on {dataset_name} with {bins} bins per channel ====\")\n",
    "        t0 = time.time()\n",
    "        # Extract features for all images; use tqdm for progress\n",
    "        features = []\n",
    "        for img in tqdm(X_images, desc=\"Extracting features\"):\n",
    "            feat = extract_color_histogram(img, bins=bins)\n",
    "            features.append(feat)\n",
    "        features = np.array(features)\n",
    "        extraction_time = time.time() - t0\n",
    "        print(f\"Feature extraction time: {extraction_time:.2f} sec\")\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, y_labels, test_size=0.3, random_state=42, stratify=y_labels\n",
    "        )\n",
    "        \n",
    "        # Train classifier and measure training time\n",
    "        clf = LogisticRegression(max_iter=500, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "        t_train = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.time() - t_train\n",
    "        print(f\"Training time: {train_time:.2f} sec\")\n",
    "        \n",
    "        # Test classifier and measure prediction time\n",
    "        t_test = time.time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        test_time = time.time() - t_test\n",
    "        print(f\"Testing time: {test_time:.2f} sec\")\n",
    "        \n",
    "        # Compute overall accuracy\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # Compute confusion matrix and classification report\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        results[bins] = {\n",
    "            \"extraction_time\": extraction_time,\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\": test_time,\n",
    "            \"accuracy\": acc,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"classification_report\": report,\n",
    "            \"model\": clf,\n",
    "        }\n",
    "        \n",
    "        # Plot the confusion matrix for visual inspection\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(f\"{dataset_name} Confusion Matrix (bins={bins})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Also display a text report\n",
    "        print(f\"Classification Report (bins={bins}):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_tiny_imagenet_experiment():\n",
    "    # Define the parquet splits (update paths if needed)\n",
    "    splits = {\n",
    "        'train': 'data/train-00000-of-00001-1359597a978bc4fa.parquet',\n",
    "        'valid': 'data/valid-00000-of-00001-70d52db3c749a935.parquet'\n",
    "    }\n",
    "    # Load training data from Tiny ImageNet (here we use the train split)\n",
    "    # Note: The dataset from Hugging Face hub is accessed via \"hf://datasets/...\"\n",
    "    try:\n",
    "        df = pd.read_parquet(\"hf://datasets/zh-plus/tiny-imagenet/\" + splits[\"train\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error loading Tiny ImageNet parquet file. Please ensure the path is correct.\")\n",
    "        raise e\n",
    "    \n",
    "    # Assume the dataframe has at least two columns: 'image' and 'label'\n",
    "    # Here we assume that the 'image' column contains raw image data that can be converted to a numpy array.\n",
    "    # (You might need to adjust this code based on the actual data format.)\n",
    "# Loading Tiny ImageNet images with proper decoding for dict format\n",
    "    X_images = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Tiny ImageNet images\"):\n",
    "        img = row['image']\n",
    "        # Check if the image is stored as a dict with keys 'bytes' and 'path'\n",
    "        if isinstance(img, dict):\n",
    "            if set(img.keys()) == {\"bytes\", \"path\"}:\n",
    "                # Decode the image bytes using PIL\n",
    "                try:\n",
    "                    pil_img = Image.open(io.BytesIO(img[\"bytes\"]))\n",
    "                    pil_img = pil_img.convert(\"RGB\")  # Ensure RGB format\n",
    "                    img = np.array(pil_img)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error decoding image bytes: {e}\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown image dict format: \" + str(img.keys()))\n",
    "        elif not isinstance(img, np.ndarray):\n",
    "            img = np.array(img)\n",
    "        X_images.append(img)\n",
    "    \n",
    "    y_labels = df['label'].tolist()\n",
    "    \n",
    "    print(f\"Tiny ImageNet: Loaded {len(X_images)} images.\")\n",
    "    \n",
    "    # Run the experiment using two histogram configurations: 8 and 16 bins per channel.\n",
    "    tiny_results = run_experiment(X_images, y_labels, dataset_name=\"Tiny ImageNet\", bins_values=[8, 16])\n",
    "    return tiny_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting experiments...\\n\")\n",
    "    \n",
    "    # Run Tiny ImageNet experiment (if the parquet file is available)\n",
    "    try:\n",
    "        tiny_results = run_tiny_imagenet_experiment()\n",
    "    except Exception as e:\n",
    "        print(\"Tiny ImageNet experiment could not be run:\", e)\n",
    "        tiny_results = None\n",
    "    \n",
    "    # Run CIFAR-10 experiment\n",
    "    cifar_results = run_cifar10_experiment()\n",
    "    \n",
    "    # (Optional) Further analysis: compare per-class errors across approaches, print runtime comparisons, etc.\n",
    "    # For example, you could compare tiny_results[8][\"accuracy\"] vs. tiny_results[16][\"accuracy\"]\n",
    "    print(\"\\nExperiments completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tiny ImageNet Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_tiny_imagenet_experiment():\n",
    "    # Define the parquet splits (update paths if needed)\n",
    "    splits = {\n",
    "        'train': 'data/train-00000-of-00001-1359597a978bc4fa.parquet',\n",
    "        'valid': 'data/valid-00000-of-00001-70d52db3c749a935.parquet'\n",
    "    }\n",
    "    # Load training data from Tiny ImageNet (here we use the train split)\n",
    "    # Note: The dataset from Hugging Face hub is accessed via \"hf://datasets/...\"\n",
    "    try:\n",
    "        df = pd.read_parquet(\"hf://datasets/zh-plus/tiny-imagenet/\" + splits[\"train\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error loading Tiny ImageNet parquet file. Please ensure the path is correct.\")\n",
    "        raise e\n",
    "    \n",
    "    # Assume the dataframe has at least two columns: 'image' and 'label'\n",
    "    # Here we assume that the 'image' column contains raw image data that can be converted to a numpy array.\n",
    "    # (You might need to adjust this code based on the actual data format.)\n",
    "# Loading Tiny ImageNet images with proper decoding for dict format\n",
    "    X_images = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Tiny ImageNet images\"):\n",
    "        img = row['image']\n",
    "        # Check if the image is stored as a dict with keys 'bytes' and 'path'\n",
    "        if isinstance(img, dict):\n",
    "            if set(img.keys()) == {\"bytes\", \"path\"}:\n",
    "                # Decode the image bytes using PIL\n",
    "                try:\n",
    "                    pil_img = Image.open(io.BytesIO(img[\"bytes\"]))\n",
    "                    pil_img = pil_img.convert(\"RGB\")  # Ensure RGB format\n",
    "                    img = np.array(pil_img)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error decoding image bytes: {e}\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown image dict format: \" + str(img.keys()))\n",
    "        elif not isinstance(img, np.ndarray):\n",
    "            img = np.array(img)\n",
    "        X_images.append(img)\n",
    "    \n",
    "    y_labels = df['label'].tolist()\n",
    "    \n",
    "    print(f\"Tiny ImageNet: Loaded {len(X_images)} images.\")\n",
    "    \n",
    "    # Run the experiment using two histogram configurations: 8 and 16 bins per channel.\n",
    "    tiny_results = run_experiment(X_images, y_labels, dataset_name=\"Tiny ImageNet\", bins_values=[8, 16])\n",
    "    return tiny_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CIFAR-10 Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_cifar10_experiment():\n",
    "    # Define a transform to convert PIL images to numpy arrays\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts to [0,1] tensor in shape (C, H, W)\n",
    "        # We'll transpose later to get HxWxC and convert to numpy\n",
    "    ])\n",
    "    \n",
    "    # Download and load CIFAR-10 training set (we will use the train split for simplicity)\n",
    "    cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    X_images = []\n",
    "    y_labels = []\n",
    "    for img, label in tqdm(cifar10_train, desc=\"Loading CIFAR-10 images\"):\n",
    "        # img is a tensor of shape (C, H, W); convert to numpy and then transpose to (H, W, C)\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        # Convert to uint8 format (0-255)\n",
    "        img_np = (img_np * 255).astype(np.uint8)\n",
    "        X_images.append(img_np)\n",
    "        y_labels.append(label)\n",
    "    \n",
    "    print(f\"CIFAR-10: Loaded {len(X_images)} images.\")\n",
    "    \n",
    "    # Run the experiment using two histogram configurations: 8 and 16 bins per channel.\n",
    "    cifar_results = run_experiment(X_images, y_labels, dataset_name=\"CIFAR-10\", bins_values=[8, 16])\n",
    "    return cifar_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main: Run Both Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Tiny ImageNet images:  26%|██▌       | 25641/100000 [00:02<00:05, 12670.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run Tiny ImageNet experiment (if the parquet file is available)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     tiny_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_tiny_imagenet_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTiny ImageNet experiment could not be run:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "Cell \u001b[0;32mIn[10], line 102\u001b[0m, in \u001b[0;36mrun_tiny_imagenet_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(img\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Decode the image bytes using PIL\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         pil_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m         pil_img \u001b[38;5;241m=\u001b[39m pil_img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure RGB format\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pil_img)\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/Image.py:3511\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3508\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3511\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[1;32m   3514\u001b[0m     checked_formats \u001b[38;5;241m=\u001b[39m ID\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/Image.py:3499\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3497\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3498\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3499\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3500\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:873\u001b[0m, in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjpeg_factory\u001b[39m(\n\u001b[1;32m    871\u001b[0m     fp: IO[\u001b[38;5;28mbytes\u001b[39m], filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    872\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JpegImageFile \u001b[38;5;241m|\u001b[39m MpoImageFile:\n\u001b[0;32m--> 873\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mJpegImageFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m         mpheader \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39m_getmp()\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/ImageFile.py:144\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    151\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mv\u001b[39;00m\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:373\u001b[0m, in \u001b[0;36mJpegImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m name, description, handler \u001b[38;5;241m=\u001b[39m MARKER[i]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFDA\u001b[39m:  \u001b[38;5;66;03m# start of scan\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[0;32m~/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:246\u001b[0m, in \u001b[0;36mDQT\u001b[0;34m(self, marker)\u001b[0m\n\u001b[1;32m    244\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (v \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# in bytes\u001b[39;00m\n\u001b[1;32m    245\u001b[0m qt_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m precision \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m qt_length:\n\u001b[1;32m    247\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad quantization table marker\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(msg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting experiments...\\n\")\n",
    "    \n",
    "    # Run Tiny ImageNet experiment (if the parquet file is available)\n",
    "    try:\n",
    "        tiny_results = run_tiny_imagenet_experiment()\n",
    "    except Exception as e:\n",
    "        print(\"Tiny ImageNet experiment could not be run:\", e)\n",
    "        tiny_results = None\n",
    "    \n",
    "    # Run CIFAR-10 experiment\n",
    "    cifar_results = run_cifar10_experiment()\n",
    "    \n",
    "    # (Optional) Further analysis: compare per-class errors across approaches, print runtime comparisons, etc.\n",
    "    # For example, you could compare tiny_results[8][\"accuracy\"] vs. tiny_results[16][\"accuracy\"]\n",
    "    print(\"\\nExperiments completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60000 FashionMNIST images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing images: 100%|██████████| 60000/60000 [00:01<00:00, 40923.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FashionMNIST: Experiment with 8 bins per image ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 60000/60000 [00:01<00:00, 30371.58it/s]\n",
      "/Users/gerhardkarbeutz/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time: 1.99 sec\n",
      "Training time: 0.45 sec\n",
      "Testing time: 0.00 sec\n",
      "Accuracy: 0.3538\n",
      "Confusion Matrix:\n",
      "[[ 174   18  205  116  205   32  436   29  438  147]\n",
      " [  13  980    8   90   10  293   13  263   26  104]\n",
      " [  74    2  687   19  368   19  420   23  158   30]\n",
      " [ 103  377   17  399   36  203   43  140   73  409]\n",
      " [  75   13  282   72  615    5  238   12  313  175]\n",
      " [  25  141    2  160    3 1205    8  215    6   35]\n",
      " [ 142    3  446   58  289   21  547   41  161   92]\n",
      " [   4  270    0   89    0  685    1  688    2   61]\n",
      " [ 157   61  302  127  287   31  166   46  313  310]\n",
      " [  61  226   27  291   94   20   18   60  242  761]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.10      0.13      1800\n",
      "           1       0.47      0.54      0.50      1800\n",
      "           2       0.35      0.38      0.36      1800\n",
      "           3       0.28      0.22      0.25      1800\n",
      "           4       0.32      0.34      0.33      1800\n",
      "           5       0.48      0.67      0.56      1800\n",
      "           6       0.29      0.30      0.30      1800\n",
      "           7       0.45      0.38      0.41      1800\n",
      "           8       0.18      0.17      0.18      1800\n",
      "           9       0.36      0.42      0.39      1800\n",
      "\n",
      "    accuracy                           0.35     18000\n",
      "   macro avg       0.34      0.35      0.34     18000\n",
      "weighted avg       0.34      0.35      0.34     18000\n",
      "\n",
      "\n",
      "=== FashionMNIST: Experiment with 16 bins per image ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 60000/60000 [00:01<00:00, 30422.71it/s]\n",
      "/Users/gerhardkarbeutz/DataScienceProjects/deep-learning-image/ex3-venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time: 1.98 sec\n",
      "Training time: 0.51 sec\n",
      "Testing time: 0.00 sec\n",
      "Accuracy: 0.3683\n",
      "Confusion Matrix:\n",
      "[[ 286   16  168  124  182    7  400   33  443  141]\n",
      " [  14 1016   11   98    7  296   13  225   23   97]\n",
      " [  81    4  707   12  362    1  438   14  152   29]\n",
      " [  96  333   17  489   30  199   51  131   76  378]\n",
      " [ 105    9  272   72  590    0  233    8  336  175]\n",
      " [  19  158    2  149    3 1207    8  207    6   41]\n",
      " [ 181    3  423   64  272    4  569   26  163   95]\n",
      " [   6  267    0   90    0  647    0  725    2   63]\n",
      " [ 198   52  295  143  267   31  167   41  301  305]\n",
      " [  77  231   23  310   82   16   15   58  248  740]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.16      0.20      1800\n",
      "           1       0.49      0.56      0.52      1800\n",
      "           2       0.37      0.39      0.38      1800\n",
      "           3       0.32      0.27      0.29      1800\n",
      "           4       0.33      0.33      0.33      1800\n",
      "           5       0.50      0.67      0.57      1800\n",
      "           6       0.30      0.32      0.31      1800\n",
      "           7       0.49      0.40      0.44      1800\n",
      "           8       0.17      0.17      0.17      1800\n",
      "           9       0.36      0.41      0.38      1800\n",
      "\n",
      "    accuracy                           0.37     18000\n",
      "   macro avg       0.36      0.37      0.36     18000\n",
      "weighted avg       0.36      0.37      0.36     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Feature Extraction Function ---\n",
    "def extract_grayscale_histogram(img, bins=16):\n",
    "    \"\"\"\n",
    "    Given a 2D image (grayscale values in [0,1]), compute a normalized histogram.\n",
    "    \"\"\"\n",
    "    # Convert to uint8 (0-255) then compute histogram\n",
    "    img_8u = (img * 255).astype(np.uint8)\n",
    "    hist, _ = np.histogram(img_8u, bins=bins, range=(0, 256))\n",
    "    hist = hist.astype(np.float32)\n",
    "    hist /= (hist.sum() + 1e-7)  # Normalize to sum to 1\n",
    "    return hist\n",
    "\n",
    "# --- Experiment Function ---\n",
    "def run_experiment(images, labels, dataset_name, bins_values=[8, 16]):\n",
    "    \"\"\"\n",
    "    Run experiments on a dataset with different histogram bin settings.\n",
    "    \n",
    "    Args:\n",
    "        images (list): List of 2D numpy arrays (grayscale images).\n",
    "        labels (list): Corresponding image labels.\n",
    "        dataset_name (str): Name of the dataset (for printing).\n",
    "        bins_values (list): List of bin counts to try.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for bins in bins_values:\n",
    "        print(f\"\\n=== {dataset_name}: Experiment with {bins} bins per image ===\")\n",
    "        \n",
    "        # Feature extraction\n",
    "        t0 = time.time()\n",
    "        features = [extract_grayscale_histogram(img, bins=bins) for img in tqdm(images, desc=\"Extracting features\")]\n",
    "        features = np.array(features)\n",
    "        extraction_time = time.time() - t0\n",
    "        print(f\"Feature extraction time: {extraction_time:.2f} sec\")\n",
    "        \n",
    "        # Split into train/test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, labels, test_size=0.3, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Train classifier\n",
    "        clf = LogisticRegression(max_iter=500, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.time() - t0\n",
    "        print(f\"Training time: {train_time:.2f} sec\")\n",
    "        \n",
    "        # Evaluate classifier\n",
    "        t0 = time.time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        test_time = time.time() - t0\n",
    "        print(f\"Testing time: {test_time:.2f} sec\")\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        results[bins] = {\n",
    "            \"extraction_time\": extraction_time,\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\": test_time,\n",
    "            \"accuracy\": acc,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"model\": clf,\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# --- Main Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load FashionMNIST\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    print(f\"Loaded {len(dataset)} FashionMNIST images.\")\n",
    "\n",
    "    # Convert images (tensor shape: [1, 28, 28]) to 2D numpy arrays\n",
    "    images, labels = [], []\n",
    "    for img_tensor, label in tqdm(dataset, desc=\"Preparing images\"):\n",
    "        images.append(img_tensor.numpy().squeeze())  # Shape: (28,28)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Run experiment with two bin configurations\n",
    "    results = run_experiment(images, labels, dataset_name=\"FashionMNIST\", bins_values=[8, 16])\n",
    "    \n",
    "    # (Optional) You can add further analysis on the `results` dictionary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex3-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
